 * Serving Flask app "airflow.utils.serve_logs" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
[[34m2025-01-12 13:50:54,536[0m] {[34mscheduler_job.py:[0m662} INFO[0m - Starting the scheduler[0m
[[34m2025-01-12 13:50:54,536[0m] {[34mscheduler_job.py:[0m667} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-01-12 13:50:54,541[0m] {[34mmanager.py:[0m254} INFO[0m - Launched DagFileProcessorManager with pid: 280964[0m
[[34m2025-01-12 13:50:54,542[0m] {[34mscheduler_job.py:[0m1217} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-01-12 13:50:54,546[0m] {[34msettings.py:[0m51} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-01-12 13:50:54,556] {manager.py:529} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2025-01-12 13:52:55,148[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12 12:51:51+00:00 [scheduled]>[0m
[[34m2025-01-12 13:52:55,151[0m] {[34mscheduler_job.py:[0m383} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2025-01-12 13:52:55,152[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG agile_data_science_batch_prediction_model_training has 0/16 running and queued tasks[0m
[[34m2025-01-12 13:52:55,152[0m] {[34mscheduler_job.py:[0m476} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12 12:51:51+00:00 [scheduled]>[0m
[[34m2025-01-12 13:52:55,155[0m] {[34mscheduler_job.py:[0m518} INFO[0m - Sending TaskInstanceKey(dag_id='agile_data_science_batch_prediction_model_training', task_id='pyspark_train_classifier_model', execution_date=datetime.datetime(2025, 1, 12, 12, 51, 51, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2025-01-12 13:52:55,156[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', '2025-01-12T12:51:51+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py'][0m
[[34m2025-01-12 13:52:55,160[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', '2025-01-12T12:51:51+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py'][0m
[[34m2025-01-12 13:52:56,699[0m] {[34mdagbag.py:[0m496} INFO[0m - Filling up the DagBag from /home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py[0m
[[34m2025-01-12 13:52:56,849[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2025-01-12 13:52:56,850[0m] {[34mexample_kubernetes_executor_config.py:[0m176} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12T12:51:51+00:00 [queued]> on host jorge-HP-Laptop-14s-dq1xxx
[[34m2025-01-12 13:53:03,646[0m] {[34mscheduler_job.py:[0m612} INFO[0m - Executor reports execution of agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model execution_date=2025-01-12 12:51:51+00:00 exited with status success for try_number 1[0m
[[34m2025-01-12 13:55:54,718[0m] {[34mscheduler_job.py:[0m1217} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-01-12 13:58:04,384[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12 12:51:51+00:00 [scheduled]>[0m
[[34m2025-01-12 13:58:04,386[0m] {[34mscheduler_job.py:[0m383} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2025-01-12 13:58:04,387[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG agile_data_science_batch_prediction_model_training has 0/16 running and queued tasks[0m
[[34m2025-01-12 13:58:04,387[0m] {[34mscheduler_job.py:[0m476} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12 12:51:51+00:00 [scheduled]>[0m
[[34m2025-01-12 13:58:04,390[0m] {[34mscheduler_job.py:[0m518} INFO[0m - Sending TaskInstanceKey(dag_id='agile_data_science_batch_prediction_model_training', task_id='pyspark_train_classifier_model', execution_date=datetime.datetime(2025, 1, 12, 12, 51, 51, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2025-01-12 13:58:04,391[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', '2025-01-12T12:51:51+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py'][0m
[[34m2025-01-12 13:58:04,398[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', '2025-01-12T12:51:51+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py'][0m
[[34m2025-01-12 13:58:06,332[0m] {[34mdagbag.py:[0m496} INFO[0m - Filling up the DagBag from /home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py[0m
[[34m2025-01-12 13:58:06,534[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2025-01-12 13:58:06,534[0m] {[34mexample_kubernetes_executor_config.py:[0m176} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12T12:51:51+00:00 [queued]> on host jorge-HP-Laptop-14s-dq1xxx
[[34m2025-01-12 14:05:45,822[0m] {[34mscheduler_job.py:[0m612} INFO[0m - Executor reports execution of agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model execution_date=2025-01-12 12:51:51+00:00 exited with status success for try_number 2[0m
[[34m2025-01-12 14:05:45,839[0m] {[34mmanager.py:[0m414} ERROR[0m - DagFileProcessorManager (PID=280964) last sent a heartbeat 461.51 seconds ago! Restarting it[0m
[[34m2025-01-12 14:05:45,846[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 280964[0m
[[34m2025-01-12 14:05:45,979[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=280964, status='terminated', exitcode=0, started='13:50:53') (280964) terminated with exit code 0[0m
[[34m2025-01-12 14:05:45,983[0m] {[34mmanager.py:[0m254} INFO[0m - Launched DagFileProcessorManager with pid: 283615[0m
[[34m2025-01-12 14:05:45,987[0m] {[34msettings.py:[0m51} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-01-12 14:05:45,997] {manager.py:529} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2025-01-12 14:05:46,009[0m] {[34mscheduler_job.py:[0m1217} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-01-12 14:05:46,011[0m] {[34mscheduler_job.py:[0m1239} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2025-01-12 14:10:45,712[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12 12:51:51+00:00 [scheduled]>[0m
[[34m2025-01-12 14:10:45,713[0m] {[34mscheduler_job.py:[0m383} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2025-01-12 14:10:45,713[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG agile_data_science_batch_prediction_model_training has 0/16 running and queued tasks[0m
[[34m2025-01-12 14:10:45,713[0m] {[34mscheduler_job.py:[0m476} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12 12:51:51+00:00 [scheduled]>[0m
[[34m2025-01-12 14:10:45,714[0m] {[34mscheduler_job.py:[0m518} INFO[0m - Sending TaskInstanceKey(dag_id='agile_data_science_batch_prediction_model_training', task_id='pyspark_train_classifier_model', execution_date=datetime.datetime(2025, 1, 12, 12, 51, 51, tzinfo=Timezone('UTC')), try_number=3) to executor with priority 1 and queue default[0m
[[34m2025-01-12 14:10:45,715[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', '2025-01-12T12:51:51+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py'][0m
[[34m2025-01-12 14:10:45,718[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', '2025-01-12T12:51:51+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py'][0m
[[34m2025-01-12 14:10:46,598[0m] {[34mdagbag.py:[0m496} INFO[0m - Filling up the DagBag from /home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py[0m
[[34m2025-01-12 14:10:46,686[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2025-01-12 14:10:46,687[0m] {[34mexample_kubernetes_executor_config.py:[0m176} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12T12:51:51+00:00 [queued]> on host jorge-HP-Laptop-14s-dq1xxx
[[34m2025-01-12 14:10:49,684[0m] {[34mscheduler_job.py:[0m612} INFO[0m - Executor reports execution of agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model execution_date=2025-01-12 12:51:51+00:00 exited with status success for try_number 3[0m
[[34m2025-01-12 14:10:49,708[0m] {[34mscheduler_job.py:[0m1217} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-01-12 14:15:50,406[0m] {[34mscheduler_job.py:[0m354} INFO[0m - 1 tasks up for execution:
	<TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12 12:51:51+00:00 [scheduled]>[0m
[[34m2025-01-12 14:15:50,411[0m] {[34mscheduler_job.py:[0m383} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2025-01-12 14:15:50,412[0m] {[34mscheduler_job.py:[0m411} INFO[0m - DAG agile_data_science_batch_prediction_model_training has 0/16 running and queued tasks[0m
[[34m2025-01-12 14:15:50,413[0m] {[34mscheduler_job.py:[0m476} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12 12:51:51+00:00 [scheduled]>[0m
[[34m2025-01-12 14:15:50,417[0m] {[34mscheduler_job.py:[0m518} INFO[0m - Sending TaskInstanceKey(dag_id='agile_data_science_batch_prediction_model_training', task_id='pyspark_train_classifier_model', execution_date=datetime.datetime(2025, 1, 12, 12, 51, 51, tzinfo=Timezone('UTC')), try_number=4) to executor with priority 1 and queue default[0m
[[34m2025-01-12 14:15:50,418[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', '2025-01-12T12:51:51+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py'][0m
[[34m2025-01-12 14:15:50,433[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'agile_data_science_batch_prediction_model_training', 'pyspark_train_classifier_model', '2025-01-12T12:51:51+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py'][0m
[[34m2025-01-12 14:15:53,146[0m] {[34mdagbag.py:[0m496} INFO[0m - Filling up the DagBag from /home/jorge/Desktop/M√ÅSTER/SEGUNDO CURSO/BDFI/practica_docker/resources/airflow/dags/train_model_dag.py[0m
[[34m2025-01-12 14:15:53,371[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2025-01-12 14:15:53,371[0m] {[34mexample_kubernetes_executor_config.py:[0m176} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model 2025-01-12T12:51:51+00:00 [queued]> on host jorge-HP-Laptop-14s-dq1xxx
[[34m2025-01-12 14:16:01,788[0m] {[34mscheduler_job.py:[0m612} INFO[0m - Executor reports execution of agile_data_science_batch_prediction_model_training.pyspark_train_classifier_model execution_date=2025-01-12 12:51:51+00:00 exited with status success for try_number 4[0m
[[34m2025-01-12 14:16:01,842[0m] {[34mscheduler_job.py:[0m1217} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-01-12 14:16:02,465[0m] {[34mdagrun.py:[0m462} ERROR[0m - Marking run <DagRun agile_data_science_batch_prediction_model_training @ 2025-01-12 12:51:51+00:00: manual__2025-01-12T12:51:51+00:00, externally triggered: True> failed[0m
[[34m2025-01-12 14:16:02,465[0m] {[34mdagrun.py:[0m647} WARNING[0m - Failed to record duration of <DagRun agile_data_science_batch_prediction_model_training @ 2025-01-12 12:51:51+00:00: manual__2025-01-12T12:51:51+00:00, externally triggered: True>: start_date is not set.[0m
